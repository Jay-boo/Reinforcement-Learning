{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env=gym.make(\"FrozenLake-v1\")\n",
    "transition_probs=env.P\n",
    "env.reset(return_info=True)\n",
    "print(env.render(\"ansi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'etats differents :16\n",
      "Shape matrice de transition :16,4\n"
     ]
    }
   ],
   "source": [
    "nb_state=env.observation_space.n\n",
    "nb_action=env.action_space.n\n",
    "print(f\"Nombre d'etats differents :{env.observation_space.n}\\n\"+\n",
    "f\"Shape matrice de transition :{len(transition_probs)},{len(transition_probs[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Policy iteration definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(P,nb_state,gamma:int=0.9,tol=10e-3):\n",
    "    V_fct=np.zeros(nb_state)\n",
    "    policy=np.zeros(nb_state,dtype=int)# dtype=int car doit renvoyé une action\n",
    "\n",
    "    flag=True\n",
    "    i=0\n",
    "    while flag and i <100:\n",
    "        print(i)\n",
    "        V_fct=policy_evaluation(P,nb_state,policy,gamma,tol)\n",
    "        new_policy=policy_improvement(P,nb_state,nb_action,V_fct,policy,gamma)\n",
    "        diff_policy=new_policy -policy\n",
    "\n",
    "        if np.linalg.norm(diff_policy)==0:\n",
    "            flag=False\n",
    "        i+=1\n",
    "\n",
    "    if (i==100):\n",
    "        print(\"Policy iteration takes too mush time to converge\")\n",
    "        exit()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def policy_evaluation(probs,nS,policy,gamma,tol):\n",
    "    \"\"\"\n",
    "    We are searching to calculate the V-function for the fixed policy \n",
    "    \"\"\"\n",
    "    \n",
    "    V_fct=np.zeros(nS)\n",
    "    error=1\n",
    "    i=0\n",
    "\n",
    "    while error > tol and i<100 :\n",
    "        new_V_fct=np.zeros(nS)\n",
    "\n",
    "        for state in range(nS):\n",
    "            # get policy for the state\n",
    "            a=policy[state]\n",
    "            transitions=probs[state][a]\n",
    "\n",
    "            for transition in transitions:\n",
    "                prob,nextState,reward,term=transition# term = Terminal state bool\n",
    "                new_V_fct[state]+=prob*(reward+gamma*V_fct[nextState])\n",
    "    \n",
    "        \n",
    "        error=np.max(np.abs(new_V_fct-V_fct))\n",
    "        V_fct=new_V_fct\n",
    "        i+=1\n",
    "    \n",
    "    if i==100:\n",
    "        print(\"Policy evaluation take too long\")\n",
    "        exit()\n",
    "    return V_fct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.0\n",
      "1\n",
      "0.0\n",
      "2\n",
      "0.0\n",
      "3\n",
      "0.0\n",
      "4\n",
      "0.0\n",
      "5\n",
      "0.0\n",
      "6\n",
      "0.0\n",
      "7\n",
      "0.0\n",
      "8\n",
      "0.0\n",
      "9\n",
      "0.0\n",
      "10\n",
      "0.0\n",
      "11\n",
      "0.0\n",
      "12\n",
      "0.0\n",
      "13\n",
      "0.0\n",
      "14\n",
      "0.0\n",
      "15\n",
      "0.0\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "policy_t=np.zeros(nb_state,dtype=int)\n",
    "nS=nb_state\n",
    "\n",
    "tol_t=10e-3\n",
    "gamma_t=0.9\n",
    "\n",
    "V_fct=np.zeros(nS)\n",
    "\n",
    "error=1\n",
    "i=0\n",
    "while error > tol_t and i<100 :\n",
    "    new_V_fct=np.zeros(nS)\n",
    "    for state in range(nS):\n",
    "        print(state)\n",
    "        # get policy for the state\n",
    "        a=policy_t[state]\n",
    "        transitions=transition_probs[state][a]\n",
    "        for transition in transitions:\n",
    "            prob,nextState,reward,term=transition# term = Terminal state bool\n",
    "            new_V_fct[state]+=prob*(reward+gamma_t*V_fct[nextState])\n",
    "        print(new_V_fct[state])\n",
    "\n",
    "    \n",
    "    error=np.max(np.abs(new_V_fct-V_fct))\n",
    "    V_fct=new_V_fct\n",
    "    i+=1\n",
    "    print(\"------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est normal que tout les state soit tel que V(S)=0  ??\n",
    "\n",
    "&rarr; En effet oui en regardant les `rewards` pour toutes les actions 0  on peut voir **que aucune action 0 depuis n'importe quelle state ne permet d'avoir de récompense**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 4, 0.0, False)],\n",
       " [(0.3333333333333333, 1, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 5, 0.0, True)],\n",
       " [(0.3333333333333333, 2, 0.0, False),\n",
       "  (0.3333333333333333, 1, 0.0, False),\n",
       "  (0.3333333333333333, 6, 0.0, False)],\n",
       " [(0.3333333333333333, 3, 0.0, False),\n",
       "  (0.3333333333333333, 2, 0.0, False),\n",
       "  (0.3333333333333333, 7, 0.0, True)],\n",
       " [(0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 4, 0.0, False),\n",
       "  (0.3333333333333333, 8, 0.0, False)],\n",
       " [(1.0, 5, 0, True)],\n",
       " [(0.3333333333333333, 2, 0.0, False),\n",
       "  (0.3333333333333333, 5, 0.0, True),\n",
       "  (0.3333333333333333, 10, 0.0, False)],\n",
       " [(1.0, 7, 0, True)],\n",
       " [(0.3333333333333333, 4, 0.0, False),\n",
       "  (0.3333333333333333, 8, 0.0, False),\n",
       "  (0.3333333333333333, 12, 0.0, True)],\n",
       " [(0.3333333333333333, 5, 0.0, True),\n",
       "  (0.3333333333333333, 8, 0.0, False),\n",
       "  (0.3333333333333333, 13, 0.0, False)],\n",
       " [(0.3333333333333333, 6, 0.0, False),\n",
       "  (0.3333333333333333, 9, 0.0, False),\n",
       "  (0.3333333333333333, 14, 0.0, False)],\n",
       " [(1.0, 11, 0, True)],\n",
       " [(1.0, 12, 0, True)],\n",
       " [(0.3333333333333333, 9, 0.0, False),\n",
       "  (0.3333333333333333, 12, 0.0, True),\n",
       "  (0.3333333333333333, 13, 0.0, False)],\n",
       " [(0.3333333333333333, 10, 0.0, False),\n",
       "  (0.3333333333333333, 13, 0.0, False),\n",
       "  (0.3333333333333333, 14, 0.0, False)],\n",
       " [(1.0, 15, 0, True)]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ transition_probs[i][0] for i in range(nS)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**evaluons une nouvelle policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end error : 0.007508699999999979\n",
      "number iteration done  : 8 \n",
      "V-function :[0.0081648  0.0072279  0.0215415  0.0075762  0.0205713  0.\n",
      " 0.0624684  0.         0.0582786  0.1496157  0.2141329  0.\n",
      " 0.         0.2430016  0.57633493 0.        ]\n"
     ]
    }
   ],
   "source": [
    "policy_t_bis=np.ones(nS,dtype=int)\n",
    "\n",
    "nS=nb_state\n",
    "\n",
    "tol_t=10e-3\n",
    "gamma_t=0.9\n",
    "\n",
    "V_fct=np.zeros(nS)\n",
    "\n",
    "error=1\n",
    "i=0\n",
    "while error > tol_t and i<100 :\n",
    "    new_V_fct=np.zeros(nS)\n",
    "    for state in range(nS):\n",
    "        \n",
    "        # get policy for the state\n",
    "        a=policy_t_bis[state]\n",
    "        transitions=transition_probs[state][a]\n",
    "        for transition in transitions:\n",
    "            prob,nextState,reward,term=transition# term = Terminal state bool\n",
    "            new_V_fct[state]+=prob*(reward+gamma_t*V_fct[nextState])\n",
    "        \n",
    "\n",
    "    \n",
    "    error=np.max(np.abs(new_V_fct-V_fct))\n",
    "    V_fct=new_V_fct\n",
    "    i+=1\n",
    "print(f\"end error : {error}\\n\"+\n",
    "f\"number iteration done  : {i} \\n\"+\n",
    "f\"V-function :{V_fct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Policy improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(probs,nS,nA,value_function_for_policy,policy,gamma):\n",
    "    new_policy=np.zeros(nS)\n",
    "\n",
    "    for state in range(nS):\n",
    "        Qs=np.zeros(nA)\n",
    "        #On va chercher l'action qui maximise la Qs à partir de l'evaluation de policy\n",
    "        for action in range(nA):\n",
    "            transitions=probs[state][action]\n",
    "            for transition in transitions:\n",
    "                prob,nextState,reward,term=transition\n",
    "                Qs[action]=prob*(reward + gamma*value_function_for_policy[nextState])\n",
    "        \n",
    "        max_action_state=np.where(Qs==Qs.max())[0]# On prend l'action qui maximise Q(s,a)\n",
    "        if(len(max_action_state>1)):\n",
    "            max_action_state=max_action_state[0]\n",
    "        new_policy[state]=max_action_state\n",
    "\n",
    "    return new_policy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testons. On repart de la  $policy =(0,0,0,....,0)$\n",
    "\n",
    "On evalue cette `policy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate V function :[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "policy_t=np.zeros(nb_state,dtype=int)\n",
    "nS=nb_state\n",
    "\n",
    "tol_t=10e-3\n",
    "gamma_t=0.9\n",
    "\n",
    "V_fct=np.zeros(nS)\n",
    "\n",
    "error=1\n",
    "i=0\n",
    "while error > tol_t and i<100 :\n",
    "    new_V_fct=np.zeros(nS)\n",
    "    for state in range(nS):\n",
    "        # get policy for the state\n",
    "        a=policy_t[state]\n",
    "        transitions=transition_probs[state][a]\n",
    "        for transition in transitions:\n",
    "            prob,nextState,reward,term=transition# term = Terminal state bool\n",
    "            new_V_fct[state]+=prob*(reward+gamma_t*V_fct[nextState])\n",
    "        \n",
    "\n",
    "    \n",
    "    error=np.max(np.abs(new_V_fct-V_fct))\n",
    "    V_fct=new_V_fct\n",
    "    i+=1\n",
    "\n",
    "print(f\"Evaluate V function :{V_fct}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de cette evaluation de cette policy on peux l'améliorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new policy improved on the policy initialize  : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "new_policy=np.zeros(nS)\n",
    "nA=nb_action\n",
    "\n",
    "for state in range(nS):\n",
    "    Qs=np.zeros(nA)\n",
    "    #On va chercher l'action qui maximise la Qs à partir de l'evaluation de policy\n",
    "    for action in range(nA):\n",
    "        transitions=transition_probs[state][action]\n",
    "        for transition in transitions:\n",
    "            prob,nextState,reward,term=transition\n",
    "            Qs[action]=prob*(reward + gamma_t*V_fct[nextState])\n",
    "    \n",
    "    max_action_state=np.where(Qs==Qs.max())[0]# On prend l'action qui maximise Q(s,a)\n",
    "    \n",
    "    if(len(max_action_state>1)):\n",
    "        max_action_state=max_action_state[0]\n",
    "    \n",
    "    new_policy[state]=max_action_state\n",
    "print(f\"The new policy improved on the policy initialize  : {new_policy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Policy iteration takes too mush time to converge\n"
     ]
    }
   ],
   "source": [
    "## Too much hard for python kernel\n",
    "# policy_iteration(transition_probs,nb_state=nb_state,tol=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "a2w = {0:'<', 1:'v', 2:'>', 3:'^'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(env.render(\"ansi\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45a851a8e049304e8d8fc9276f397ed5699faa42a654317800ed71714fc34a6c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
