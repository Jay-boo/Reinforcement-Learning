{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Policy and Value Iteration in the FrozenLake `env`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of content:\n",
    "- [Policy iteration](#id-policyiteration)\n",
    "    - [Policy iteration definition](#id-section1)\n",
    "    - [Policy evaluation](#id-section2)\n",
    "    - [Policy improvement](#id-section3)\n",
    "    - [Vizualisation](#id-section4)\n",
    "- [Value iteration](#id-valueiteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env=gym.make(\"FrozenLake-v1\")\n",
    "transition_probs=env.P\n",
    "env.reset(return_info=True)\n",
    "print(env.render(\"ansi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'etats differents :16\n",
      "Shape matrice de transition :16,4\n"
     ]
    }
   ],
   "source": [
    "nb_state=env.observation_space.n\n",
    "nb_action=env.action_space.n\n",
    "print(f\"Nombre d'etats differents :{env.observation_space.n}\\n\"+\n",
    "f\"Shape matrice de transition :{len(transition_probs)},{len(transition_probs[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"id-policyiteration\">\n",
    "\n",
    "# Policy Iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"id-section1\">\n",
    "\n",
    "## 1.Policy iteration definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(P,nb_state,gamma:int=0.9,tol=10e-3):\n",
    "    V_fct=np.zeros(nb_state)\n",
    "    policy=np.zeros(nb_state,dtype=int)# dtype=int car doit renvoyé une action\n",
    "\n",
    "    flag=True\n",
    "    i=0\n",
    "    while flag and i <100:\n",
    "        print(i)\n",
    "        V_fct=policy_evaluation(P,nb_state,policy,gamma,tol)\n",
    "        new_policy=policy_improvement(P,nb_state,nb_action,V_fct,policy,gamma)\n",
    "        diff_policy=new_policy -policy\n",
    "\n",
    "        if np.linalg.norm(diff_policy)==0:\n",
    "            flag=False\n",
    "        i+=1\n",
    "        policy=new_policy\n",
    "\n",
    "    if (i==100):\n",
    "        print(\"Policy iteration takes too mush time to converge\")\n",
    "        exit()\n",
    "    return policy\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"id-section2\">\n",
    "\n",
    "## 2. Policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def policy_evaluation(probs,nS,policy,gamma,tol):\n",
    "    \"\"\"\n",
    "    We are searching to calculate the V-function for the fixed policy \n",
    "    \"\"\"\n",
    "    \n",
    "    V_fct=np.zeros(nS)\n",
    "    error=1\n",
    "    i=0\n",
    "\n",
    "    while error > tol and i<100 :\n",
    "        new_V_fct=np.zeros(nS)\n",
    "\n",
    "        for state in range(nS):\n",
    "            # get policy for the state\n",
    "            a=policy[state]\n",
    "            transitions=probs[state][a]\n",
    "\n",
    "            for transition in transitions:\n",
    "                prob,nextState,reward,term=transition# term = Terminal state bool\n",
    "                new_V_fct[state]+=prob*(reward+gamma*V_fct[nextState])\n",
    "    \n",
    "        \n",
    "        error=np.max(np.abs(new_V_fct-V_fct))\n",
    "        V_fct=new_V_fct\n",
    "        i+=1\n",
    "    \n",
    "    if i==100:\n",
    "        print(\"Policy evaluation take too long\")\n",
    "        exit()\n",
    "    return V_fct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.0\n",
      "1\n",
      "0.0\n",
      "2\n",
      "0.0\n",
      "3\n",
      "0.0\n",
      "4\n",
      "0.0\n",
      "5\n",
      "0.0\n",
      "6\n",
      "0.0\n",
      "7\n",
      "0.0\n",
      "8\n",
      "0.0\n",
      "9\n",
      "0.0\n",
      "10\n",
      "0.0\n",
      "11\n",
      "0.0\n",
      "12\n",
      "0.0\n",
      "13\n",
      "0.0\n",
      "14\n",
      "0.0\n",
      "15\n",
      "0.0\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "policy_t=np.zeros(nb_state,dtype=int)\n",
    "nS=nb_state\n",
    "\n",
    "tol_t=10e-3\n",
    "gamma_t=0.9\n",
    "\n",
    "V_fct=np.zeros(nS)\n",
    "\n",
    "error=1\n",
    "i=0\n",
    "while error > tol_t and i<100 :\n",
    "    new_V_fct=np.zeros(nS)\n",
    "    for state in range(nS):\n",
    "        print(state)\n",
    "        # get policy for the state\n",
    "        a=policy_t[state]\n",
    "        transitions=transition_probs[state][a]\n",
    "        for transition in transitions:\n",
    "            prob,nextState,reward,term=transition# term = Terminal state bool\n",
    "            new_V_fct[state]+=prob*(reward+gamma_t*V_fct[nextState])\n",
    "        print(new_V_fct[state])\n",
    "\n",
    "    \n",
    "    error=np.max(np.abs(new_V_fct-V_fct))\n",
    "    V_fct=new_V_fct\n",
    "    i+=1\n",
    "    print(\"------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est normal que tout les state soit tel que V(S)=0  ??\n",
    "\n",
    "&rarr; En effet oui en regardant les `rewards` pour toutes les actions 0  on peut voir **que aucune action 0 depuis n'importe quelle state ne permet d'avoir de récompense**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 4, 0.0, False)],\n",
       " [(0.3333333333333333, 1, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 5, 0.0, True)],\n",
       " [(0.3333333333333333, 2, 0.0, False),\n",
       "  (0.3333333333333333, 1, 0.0, False),\n",
       "  (0.3333333333333333, 6, 0.0, False)],\n",
       " [(0.3333333333333333, 3, 0.0, False),\n",
       "  (0.3333333333333333, 2, 0.0, False),\n",
       "  (0.3333333333333333, 7, 0.0, True)],\n",
       " [(0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 4, 0.0, False),\n",
       "  (0.3333333333333333, 8, 0.0, False)],\n",
       " [(1.0, 5, 0, True)],\n",
       " [(0.3333333333333333, 2, 0.0, False),\n",
       "  (0.3333333333333333, 5, 0.0, True),\n",
       "  (0.3333333333333333, 10, 0.0, False)],\n",
       " [(1.0, 7, 0, True)],\n",
       " [(0.3333333333333333, 4, 0.0, False),\n",
       "  (0.3333333333333333, 8, 0.0, False),\n",
       "  (0.3333333333333333, 12, 0.0, True)],\n",
       " [(0.3333333333333333, 5, 0.0, True),\n",
       "  (0.3333333333333333, 8, 0.0, False),\n",
       "  (0.3333333333333333, 13, 0.0, False)],\n",
       " [(0.3333333333333333, 6, 0.0, False),\n",
       "  (0.3333333333333333, 9, 0.0, False),\n",
       "  (0.3333333333333333, 14, 0.0, False)],\n",
       " [(1.0, 11, 0, True)],\n",
       " [(1.0, 12, 0, True)],\n",
       " [(0.3333333333333333, 9, 0.0, False),\n",
       "  (0.3333333333333333, 12, 0.0, True),\n",
       "  (0.3333333333333333, 13, 0.0, False)],\n",
       " [(0.3333333333333333, 10, 0.0, False),\n",
       "  (0.3333333333333333, 13, 0.0, False),\n",
       "  (0.3333333333333333, 14, 0.0, False)],\n",
       " [(1.0, 15, 0, True)]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ transition_probs[i][0] for i in range(nS)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**evaluons une nouvelle policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end error : 0.007508699999999979\n",
      "number iteration done  : 8 \n",
      "V-function :[0.0081648  0.0072279  0.0215415  0.0075762  0.0205713  0.\n",
      " 0.0624684  0.         0.0582786  0.1496157  0.2141329  0.\n",
      " 0.         0.2430016  0.57633493 0.        ]\n"
     ]
    }
   ],
   "source": [
    "policy_t_bis=np.ones(nS,dtype=int)\n",
    "\n",
    "nS=nb_state\n",
    "\n",
    "tol_t=10e-3\n",
    "gamma_t=0.9\n",
    "\n",
    "V_fct=np.zeros(nS)\n",
    "\n",
    "error=1\n",
    "i=0\n",
    "while error > tol_t and i<100 :\n",
    "    new_V_fct=np.zeros(nS)\n",
    "    for state in range(nS):\n",
    "        \n",
    "        # get policy for the state\n",
    "        a=policy_t_bis[state]\n",
    "        transitions=transition_probs[state][a]\n",
    "        for transition in transitions:\n",
    "            prob,nextState,reward,term=transition# term = Terminal state bool\n",
    "            new_V_fct[state]+=prob*(reward+gamma_t*V_fct[nextState])\n",
    "        \n",
    "\n",
    "    \n",
    "    error=np.max(np.abs(new_V_fct-V_fct))\n",
    "    V_fct=new_V_fct\n",
    "    i+=1\n",
    "print(f\"end error : {error}\\n\"+\n",
    "f\"number iteration done  : {i} \\n\"+\n",
    "f\"V-function :{V_fct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"id-section3\">\n",
    "\n",
    "## 3. Policy improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(probs,nS,nA,value_function_for_policy,policy,gamma):\n",
    "    new_policy=np.zeros(nS)\n",
    "\n",
    "    for state in range(nS):\n",
    "        Qs=np.zeros(nA)\n",
    "        #On va chercher l'action qui maximise la Qs à partir de l'evaluation de policy\n",
    "        for action in range(nA):\n",
    "            transitions=probs[state][action]\n",
    "            for transition in transitions:\n",
    "                prob,nextState,reward,term=transition\n",
    "                Qs[action]=prob*(reward + gamma*value_function_for_policy[nextState])\n",
    "        \n",
    "        max_action_state=np.where(Qs==Qs.max())[0]# On prend l'action qui maximise Q(s,a)\n",
    "        if(len(max_action_state>1)):\n",
    "            max_action_state=max_action_state[0]\n",
    "        new_policy[state]=max_action_state\n",
    "\n",
    "    return new_policy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testons. On repart de la  $policy =(0,0,0,....,0)$\n",
    "\n",
    "On evalue cette `policy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate V function :[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "policy_t=np.zeros(nb_state,dtype=int)\n",
    "nS=nb_state\n",
    "\n",
    "tol_t=10e-3\n",
    "gamma_t=0.9\n",
    "\n",
    "V_fct=np.zeros(nS)\n",
    "\n",
    "error=1\n",
    "i=0\n",
    "while error > tol_t and i<100 :\n",
    "    new_V_fct=np.zeros(nS)\n",
    "    for state in range(nS):\n",
    "        # get policy for the state\n",
    "        a=policy_t[state]\n",
    "        transitions=transition_probs[state][a]\n",
    "        for transition in transitions:\n",
    "            prob,nextState,reward,term=transition# term = Terminal state bool\n",
    "            new_V_fct[state]+=prob*(reward+gamma_t*V_fct[nextState])\n",
    "        \n",
    "\n",
    "    \n",
    "    error=np.max(np.abs(new_V_fct-V_fct))\n",
    "    V_fct=new_V_fct\n",
    "    i+=1\n",
    "\n",
    "print(f\"Evaluate V function :{V_fct}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de cette evaluation de cette policy on peux l'améliorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new policy improved on the policy initialize  : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "new_policy=np.zeros(nS)\n",
    "nA=nb_action\n",
    "\n",
    "for state in range(nS):\n",
    "    Qs=np.zeros(nA)\n",
    "    #On va chercher l'action qui maximise la Qs à partir de l'evaluation de policy\n",
    "    for action in range(nA):\n",
    "        transitions=transition_probs[state][action]\n",
    "        for transition in transitions:\n",
    "            prob,nextState,reward,term=transition\n",
    "            Qs[action]=prob*(reward + gamma_t*V_fct[nextState])\n",
    "    \n",
    "    max_action_state=np.where(Qs==Qs.max())[0]# On prend l'action qui maximise Q(s,a)\n",
    "    \n",
    "    if(len(max_action_state>1)):\n",
    "        max_action_state=max_action_state[0]\n",
    "    \n",
    "    new_policy[state]=max_action_state\n",
    "print(f\"The new policy improved on the policy initialize  : {new_policy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "## Too much hard for python kernel\n",
    "best_policy=policy_iteration(transition_probs,nb_state=nb_state,tol=10e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally obtaining our policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<', 'v', '<', '^', '<', '<', '<', '<', 'v', '<', '<', '<', '<', 'v', 'v', '<']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "action2arrow = {0:'<', 1:'v', 2:'>', 3:'^'}\n",
    "arrow_policy=[action2arrow[action] for action in best_policy]\n",
    "print(arrow_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"id-section4\">\n",
    "\n",
    "## 4. Vizualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_frames(frames):\n",
    "    for i , frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Timestep: 9\n"
     ]
    }
   ],
   "source": [
    "env.reset(return_info=True)\n",
    "frames=[]\n",
    "frames.append({'frame':env.render(\"ansi\")})\n",
    "for s in range(20):\n",
    "    \n",
    "    action=env.action_space.sample()\n",
    "    state,reward,done,info=env.step(action)\n",
    "    frames.append({'frame':env.render(\"ansi\")})\n",
    "    \n",
    "    if done:\n",
    "        print(\"end\")\n",
    "        break\n",
    "print_frames(frames=frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n",
      "\n",
      "Timestep: 7\n"
     ]
    }
   ],
   "source": [
    "obs=env.reset()\n",
    "state=obs\n",
    "frames=[]\n",
    "frames.append({'frame':env.render(\"ansi\")})\n",
    "\n",
    "for s in range(10):\n",
    "    \n",
    "    action=int(best_policy[state])\n",
    "    state,reward,done,info=env.step(action)\n",
    "    frames.append({'frame':env.render(\"ansi\")})\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "print_frames(frames=frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"id-valueiteration\">\n",
    "\n",
    "# Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Désormais au lieux de à chaque fois évaluer la policy et ensuite l'améliorer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(P,nS,nA,gamma=0.9,tol=1e-3):\n",
    "    \n",
    "    V_fct=np.zeros(nS)\n",
    "    policy = np.zeros(nS,dtype=int)\n",
    "    \n",
    "    \n",
    "    error=1\n",
    "    # Iterate Value function  until finding the optimal V function\n",
    "    while error > tol :\n",
    "        new_V_fct=np.zeros(nS)\n",
    "        for s in range(nS):\n",
    "            Qs=np.zeros(nA)\n",
    "            for  a in range(nA):\n",
    "                transitions=P[s][a]\n",
    "                for transition in transitions:\n",
    "                    prob,nextState,reward,term=transition\n",
    "                    Qs[a]+=prob*(reward+gamma*V_fct[nextState])\n",
    "            new_V_fct[s]=max(Qs)\n",
    "        diff_vf=new_V_fct - V_fct\n",
    "        V_fct=new_V_fct\n",
    "        error=np.linalg.norm(diff_vf)\n",
    "\n",
    "    # Now than we find the optimal V function we can get the optimal polic\n",
    "    #Just need to recalculate each Qs optimal function and just take the max \n",
    "    \n",
    "\n",
    "    for s in range(nS):\n",
    "        Qs=np.zeros(nA)\n",
    "        for a in range(nA):\n",
    "            transitions=P[s][a]\n",
    "            for transition in transitions:\n",
    "                prob, nextState,reward,term = transition\n",
    "                Qs[a]+= prob*(reward+gamma*V_fct[nextState])\n",
    "            \n",
    "        max_action_state=np.where(Qs==Qs.max())[0]\n",
    "        if(len(max_action_state>1)):\n",
    "            max_action_state=max_action_state[0]\n",
    "        policy[s]=max_action_state\n",
    "\n",
    "    return V_fct, policy\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_fct_opt,policy_opt=value_iteration(transition_probs,nb_state,nb_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Optimal V function[0.06605707 0.0590205  0.07266446 0.05390168 0.08927196 0.\n",
      " 0.11126389 0.         0.14333674 0.24607285 0.29861482 0.\n",
      " 0.         0.37890075 0.63847528 0.        ] \n",
      "OPtimal Policy :[0 3 0 3 0 0 0 0 3 1 0 0 0 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(f' Optimal V function{V_fct_opt} \\n'+f\"OPtimal Policy :{policy_opt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With value iteration : ['<', '^', '<', '^', '<', '<', '<', '<', '^', 'v', '<', '<', '<', '>', 'v', '<'] \n",
      "  With policy iteration : ['<', 'v', '<', '^', '<', '<', '<', '<', 'v', '<', '<', '<', '<', 'v', 'v', '<']\n"
     ]
    }
   ],
   "source": [
    "arrow_policy_opt=[action2arrow[action] for action in policy_opt]\n",
    "print(f\"With value iteration : {arrow_policy_opt} \\n  With policy iteration : {arrow_policy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45a851a8e049304e8d8fc9276f397ed5699faa42a654317800ed71714fc34a6c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
