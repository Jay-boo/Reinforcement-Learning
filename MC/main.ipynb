{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://twice22.github.io/rl-part4/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env=gym.make(\"Blackjack-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Discrete(32), Discrete(11), Discrete(2))\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tuples rpresent the states :\n",
    "\n",
    "&rarr;The first element is the current player's sum $ {0,1,....,31 } $\n",
    "\n",
    "&rarr; The second is the dealer's face up card ${1,...,10}$\n",
    "\n",
    "&rarr; The plays has an usuale ace $0,1$. O:NO , 1:YES\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17, 10, False), {})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset(return_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En terme d'action possible : Soit on retire une carte, soit l'on ne fait rien :\n",
    "\n",
    " - 1 : HIT. On tire une carte\n",
    " - 2 : Stick. ON ne tire pas de carte "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22, 10, False), -1.0, True, {})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let launch 4 parties with random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 10, True)\n",
      "(15, 10, True)\n",
      "(15, 10, False)\n",
      "(18, 10, False)\n",
      "lose\n",
      "\n",
      "(21, 2, True)\n",
      "(21, 2, False)\n",
      "win\n",
      "(21, 6, True)\n",
      "(13, 6, False)\n",
      "(18, 6, False)\n",
      "lose\n",
      "\n",
      "(12, 4, False)\n",
      "lose\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for _ in range(4):\n",
    "    state=env.reset()\n",
    "    while True :\n",
    "        print(state)\n",
    "        a=env.action_space.sample()\n",
    "        state,reward,term,info=env.step(a)\n",
    "        if term :\n",
    "            print(\"win\" if reward >0 else \"lose\"+\"\\n\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_deterministic(env):\n",
    "    state=env.reset()\n",
    "    episode=[]\n",
    "    while True:\n",
    "        action=int(state[0]<=18)#Our deterministic policy\n",
    "        nextState,reward,term,info=env.step(action)\n",
    "        episode.append((state,action,reward))\n",
    "        state=nextState\n",
    "        if term :\n",
    "            break\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte-Carlo  Value estimation first visit:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCV(env,generate_episode,gamma,episodes):\n",
    "    V,returns={},{}\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        trajectory = generate_episode(env)\n",
    "        G,T=0,len(trajectory)\n",
    "        visited_state=set()\n",
    "\n",
    "        for i in range(T-1,-1,-1):\n",
    "            state,action,reward=trajectory[i]\n",
    "            G+=gamma**i * reward\n",
    "            if state not in visited_state :\n",
    "                reward, visits = returns.get(state,[0,0])\n",
    "                returns[state]= [reward + G ,visits +1]\n",
    "                visited_state.add(state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45a851a8e049304e8d8fc9276f397ed5699faa42a654317800ed71714fc34a6c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
